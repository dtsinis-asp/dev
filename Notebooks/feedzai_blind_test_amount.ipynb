{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieltsinis/Documents/sandbox/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3051: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "blind_test = pd.read_csv(\"Result_3.csv\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create cols \n",
    "\n",
    "risk_level = ['RISK_600','RISK_700','RISK_800','RISK_900']\n",
    "tenure_buckets = ['0-25','51-100','101-200','201-300','301-400','401-500','501-1000','1000+']\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for risk in risk_level:\n",
    "    for bucket in tenure_buckets:\n",
    "\n",
    "        eval_df = blind_test[blind_test['TENURE_BUCKETS']==bucket]\n",
    "    #     eval_df = blind_test[['TRANSACTION_ID','LABEL','AMOUNT','RISK_600']]\n",
    "\n",
    "        eval_df_tp_amount = eval_df[(eval_df['LABEL']==1)&(eval_df[risk]==1)]['AMOUNT'].sum()\n",
    "        eval_df_tn_amount = eval_df[(eval_df['LABEL']==0)&(eval_df[risk]==0)]['AMOUNT'].sum()\n",
    "        eval_df_fp_amount = eval_df[(eval_df['LABEL']==0)&(eval_df[risk]==1)]['AMOUNT'].sum()\n",
    "        eval_df_fn_amount = eval_df[(eval_df['LABEL']==1)&(eval_df[risk]==0)]['AMOUNT'].sum()\n",
    "\n",
    "        CM = confusion_matrix(eval_df['LABEL'], eval_df[risk])\n",
    "        TN = CM[0][0]\n",
    "        FN = CM[1][0]\n",
    "        TP = CM[1][1]\n",
    "        FP = CM[0][1]\n",
    "        \n",
    "        d = [{'Risk': risk,\n",
    "              'Bucket':str(bucket),\n",
    "              'Size': str(len(eval_df)),\n",
    "              'Accuracy':accuracy_score(eval_df['LABEL'],eval_df[risk]),\n",
    "              'Precision':precision_score(eval_df['LABEL'],eval_df[risk]),\n",
    "              'Recall':recall_score(eval_df['LABEL'],eval_df[risk]),\n",
    "              'False Positive Rate': (FP/(FP+TN)),\n",
    "              \n",
    "              'Number of True Negatives': TN,\n",
    "              'Number of True Positives': TP,\n",
    "              'Number of False Negatives': FN,\n",
    "              'Number of False Negatives': FP,\n",
    "              'TP Cost':eval_df_tp_amount,\n",
    "              'TN Cost':eval_df_tn_amount,\n",
    "              'FP Cost':eval_df_fp_amount,\n",
    "              'FN Cost':eval_df_fn_amount\n",
    "             }]\n",
    "  \n",
    "        iter_df = pd.DataFrame(d)\n",
    "        final_df = final_df.append(iter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('amount_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
